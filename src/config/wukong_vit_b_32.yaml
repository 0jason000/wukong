model:
  visual:
    type: VisionTransformer
    input_resolution: 224
    layers: 12
    width: 768
    patch_size: 32
    output_dim: 256
    token_learner:
      num_tokens: 12
      num_groups: 8
      dropout_rate: 0.0
  text:
    type: TextTransformer
    context_length: 32
    vocab_size: 21128
    width: 512
    heads: 8
    layers: 12
    output_dim: 256
eval: filip
