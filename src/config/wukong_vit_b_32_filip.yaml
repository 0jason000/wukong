model:
  visual:
    type: VisionTransformer
    input_resolution: 224
    layers: 12
    width: 768
    patch_size: 32
    output_dim: 256
  text:
    type: TextTransformer
    context_length: 32
    vocab_size: 21128
    width: 512
    heads: 8
    layers: 12
    output_dim: 256
eval: filip
